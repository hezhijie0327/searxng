# SPDX-License-Identifier: AGPL-3.0-or-later
# pylint: disable=missing-module-docstring, missing-class-docstring, protected-access
from __future__ import annotations
import math
import re
import typing
import hashlib
import time
import os
from functools import lru_cache
from threading import Lock
from pathlib import Path
import logging

import numpy as np
import bm25s
import bm25s.stopwords as stopwords_module

from searx.plugins import Plugin, PluginInfo
from searx.result_types import EngineResults

# Lazy import for sentence-transformers to avoid startup overhead
sentence_transformers = None
torch = None

if typing.TYPE_CHECKING:
    from searx.search import SearchWithPlugins
    from searx.extended_types import SXNG_Request
    from searx.plugins import PluginCfg

logger = logging.getLogger(__name__)

# ğŸš€ é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥æå‡æ€§èƒ½
HTML_TAG_RE = re.compile(r'<[^>]+>')
WHITESPACE_RE = re.compile(r'\s+')
URL_WORD_RE = re.compile(r'[a-zA-Z]+')

# ğŸ“ æ¨¡å‹è·¯å¾„é…ç½®
DEFAULT_MODEL_DIR = os.environ.get('SEARXNG_MODEL_DIR', '/var/cache/searxng/models')
DEFAULT_MODEL_NAME = "all-MiniLM-L6-v2"

class ScoreDetails:
    """è½»é‡çº§åˆ†æ•°è¯¦æƒ…ç±»"""
    __slots__ = [
        'doc_index',
        'title',
        'bm25_raw',
        'bm25_normalized',
        'semantic_raw',
        'semantic_normalized',
        'combined_score',
        'final_position',
        'position_multiplier',
    ]

    def __init__(self, doc_index: int, title: str = ""):
        self.doc_index = doc_index
        self.title = title[:40] + "..." if len(title) > 40 else title  # å‡å°‘å­—ç¬¦æ•°
        self.bm25_raw = 0.0
        self.bm25_normalized = 0.0
        self.semantic_raw = 0.0
        self.semantic_normalized = 0.0
        self.combined_score = 0.0
        self.final_position = 0.0
        self.position_multiplier = 1.0


class FastSemanticCache:
    """é«˜æ€§èƒ½è¯­ä¹‰å‘é‡ç¼“å­˜"""

    def __init__(self, max_size: int = 800, ttl: int = 3600):
        self.cache = {}
        self.access_times = {}  # ç®€åŒ–æ—¶é—´æˆ³ç®¡ç†
        self.max_size = max_size
        self.ttl = ttl
        self.lock = Lock()
        self.hit_count = 0
        self.miss_count = 0

        logger.info(f"âš¡ FastSemanticCache initialized: {max_size} entries")

    @lru_cache(maxsize=2048)
    def _generate_key(self, text: str) -> str:
        """ç¼“å­˜çš„é”®ç”Ÿæˆ"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()[:16]  # ç¼©çŸ­keyé•¿åº¦

    def get(self, text: str) -> np.ndarray | None:
        """å¿«é€Ÿè·å–ç¼“å­˜çš„å‘é‡"""
        key = self._generate_key(text)
        current_time = time.time()

        with self.lock:
            if key in self.cache:
                # ç®€åŒ–è¿‡æœŸæ£€æŸ¥
                if current_time - self.access_times.get(key, 0) < self.ttl:
                    self.hit_count += 1
                    self.access_times[key] = current_time
                    return self.cache[key]
                else:
                    # å¿«é€Ÿæ¸…ç†
                    del self.cache[key]
                    del self.access_times[key]

        self.miss_count += 1
        return None

    def set(self, text: str, vector: np.ndarray) -> None:
        """å¿«é€Ÿè®¾ç½®ç¼“å­˜"""
        key = self._generate_key(text)
        current_time = time.time()

        with self.lock:
            # ç®€åŒ–ç¼“å­˜æ»¡å¤„ç†
            if len(self.cache) >= self.max_size:
                # åˆ é™¤æœ€æ—§çš„3ä¸ªæ¡ç›®ï¼ˆæ‰¹é‡æ¸…ç†ï¼‰
                old_keys = sorted(self.access_times.items(), key=lambda x: x[1])[:3]
                for old_key, _ in old_keys:
                    self.cache.pop(old_key, None)
                    self.access_times.pop(old_key, None)

            self.cache[key] = vector
            self.access_times[key] = current_time

    def get_hit_rate(self) -> float:
        """å¿«é€Ÿè·å–å‘½ä¸­ç‡"""
        total = self.hit_count + self.miss_count
        return (self.hit_count / total * 100) if total > 0 else 0


class ModelManager:
    """ä¼˜åŒ–çš„æ¨¡å‹ç®¡ç†å™¨ - ç®€åŒ–é€»è¾‘ï¼šæœ¬åœ°ä¸å­˜åœ¨å°±ä¸‹è½½"""

    def __init__(self, model_dir: str = DEFAULT_MODEL_DIR, model_name: str = DEFAULT_MODEL_NAME):
        self.model_dir = Path(model_dir)
        self.model_name = model_name
        self.local_model_path = self.model_dir / model_name

        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        self.model_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"ğŸ“ Model manager initialized:")
        logger.info(f"   - Model dir: {self.model_dir}")
        logger.info(f"   - Model name: {self.model_name}")
        logger.info(f"   - Local path: {self.local_model_path}")

    def _is_model_valid(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å®Œæ•´æœ‰æ•ˆ"""
        if not self.local_model_path.exists():
            return False

        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ['config.json']
        model_files = ['pytorch_model.bin', 'model.safetensors']

        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        if not (self.local_model_path / 'config.json').exists():
            return False

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªï¼‰
        has_model_file = any((self.local_model_path / f).exists() for f in model_files)
        if not has_model_file:
            return False

        logger.debug(f"âœ… Local model validated: {self.local_model_path}")
        return True

    def _download_model_to_local(self) -> bool:
        """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•"""
        try:
            logger.info(f"â¬‡ï¸ Downloading model '{self.model_name}' to {self.local_model_path}")

            # å¯¼å…¥sentence_transformers
            global sentence_transformers
            if sentence_transformers is None:
                import sentence_transformers

            # ä»HuggingFaceä¸‹è½½æ¨¡å‹
            model = sentence_transformers.SentenceTransformer(self.model_name)

            # ä¿å­˜åˆ°æœ¬åœ°ç›®å½•
            model.save(str(self.local_model_path))

            # éªŒè¯ä¸‹è½½ç»“æœ
            if self._is_model_valid():
                logger.info(f"âœ… Model successfully downloaded and saved to {self.local_model_path}")
                return True
            else:
                logger.error(f"âŒ Downloaded model validation failed")
                return False

        except Exception as e:
            logger.error(f"âŒ Failed to download model: {e}")
            return False

    def ensure_model_available(self) -> str:
        """ç¡®ä¿æ¨¡å‹å¯ç”¨ï¼Œè¿”å›æœ¬åœ°æ¨¡å‹è·¯å¾„"""
        # 1. æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        if self._is_model_valid():
            logger.debug(f"ğŸ“ Using existing local model: {self.local_model_path}")
            return str(self.local_model_path)

        # 2. æœ¬åœ°ä¸å­˜åœ¨ï¼Œä¸‹è½½åˆ°æœ¬åœ°
        logger.info(f"ğŸ“¥ Local model not found, downloading...")
        if self._download_model_to_local():
            return str(self.local_model_path)

        # 3. ä¸‹è½½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise RuntimeError(f"Failed to ensure model availability: {self.model_name}")

    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            'model_name': self.model_name,
            'model_dir': str(self.model_dir),
            'local_path': str(self.local_model_path),
            'exists': self.local_model_path.exists(),
            'valid': self._is_model_valid(),
        }

        if self.local_model_path.exists():
            try:
                # è·å–æ¨¡å‹å¤§å°
                total_size = sum(f.stat().st_size for f in self.local_model_path.rglob('*') if f.is_file())
                info['size_mb'] = round(total_size / (1024 * 1024), 2)
                info['file_count'] = len(list(self.local_model_path.rglob('*')))
            except Exception:
                info['size_mb'] = 0
                info['file_count'] = 0

        return info


class SXNGPlugin(Plugin):
    """Hybrid Rerank: CPU-optimized semantic + BM25 fusion plugin"""

    id = "hybrid_rerank"
    default_on = True

    def __init__(self, plg_cfg: "PluginCfg") -> None:
        super().__init__(plg_cfg)

        # ğŸš€ æ··åˆæƒé‡é…ç½®ï¼šè¯­ä¹‰ä¼˜å…ˆ
        self.semantic_weight = 0.75
        self.bm25_weight = 0.25

        # ğŸ“ æ¨¡å‹ç®¡ç† - ä½¿ç”¨ä¼˜åŒ–çš„é€»è¾‘
        self.model_manager = ModelManager(
            model_dir=os.environ.get('SEARXNG_MODEL_DIR', DEFAULT_MODEL_DIR),
            model_name=os.environ.get('SEARXNG_MODEL_NAME', DEFAULT_MODEL_NAME),
        )

        # ğŸ”¥ CPUæ€§èƒ½ä¼˜åŒ–çš„æ‰¹å¤„ç†é…ç½®
        self.cpu_batch_size = 16  # é™ä½åˆ°CPUå‹å¥½çš„å¤§å°
        self.cpu_max_batch_size = 32  # æœ€å¤§æ‰¹é‡é™åˆ¶
        self.cpu_chunk_size = 8  # å†…å­˜å‹å¥½çš„å—å¤§å°
        self.max_results_limit = 200  # é™ä½å¤„ç†é™åˆ¶
        self.max_length = 200  # å‡å°‘æ–‡æœ¬é•¿åº¦

        # ğŸš€ æ€§èƒ½æ§åˆ¶
        self.cpu_timeout = 20.0  # æ›´ä¸¥æ ¼çš„è¶…æ—¶
        self.performance_threshold = 8.0  # æ›´ä¸¥æ ¼çš„æ€§èƒ½é˜ˆå€¼
        self.enable_fast_mode = True  # å¯ç”¨å¿«é€Ÿæ¨¡å¼

        # ğŸ“Š ç®€åŒ–æ˜¾ç¤ºé…ç½®
        self.show_detailed_scores = True
        self.show_top_n_scores = 8  # å‡å°‘æ˜¾ç¤ºæ•°é‡
        self.show_score_distribution = False  # å…³é—­åˆ†å¸ƒç»Ÿè®¡ä»¥èŠ‚çœCPU
        self.enable_debug_logs = False  # å…³é—­è¯¦ç»†è°ƒè¯•æ—¥å¿—

        # è¿è¡Œæ—¶çŠ¶æ€
        self._model = None
        self._model_load_failed = False
        self._semantic_cache = FastSemanticCache(max_size=800, ttl=3600)
        self._performance_stats = {
            "total_calls": 0,
            "avg_time": 0,
            "cpu_optimized_calls": 0,
            "fast_mode_calls": 0,
            "timeout_fallbacks": 0,
        }

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_info = self.model_manager.get_model_info()

        logger.info(f"ğŸ”„ HYBRID RERANK Plugin initialized:")
        logger.info(f"   - Semantic weight: {self.semantic_weight} ({self.semantic_weight*100:.0f}%)")
        logger.info(f"   - BM25 weight: {self.bm25_weight} ({self.bm25_weight*100:.0f}%)")
        logger.info(f"   - CPU batch size: {self.cpu_batch_size}")
        logger.info(f"   - Max results: {self.max_results_limit}")
        logger.info(f"   - Fast mode: {self.enable_fast_mode}")
        logger.info(f"   - Model status: {'âœ… Valid' if model_info['valid'] else 'â³ Will download on first use'}")

        if model_info['valid']:
            logger.info(f"   - Model size: {model_info.get('size_mb', 0):.1f} MB")

        self.info = PluginInfo(
            id=self.id,
            name="Hybrid Rerank",
            description="A hybrid reranking method that combines traditional information retrieval with modern semantic understanding.",
            preference_section="general",
        )

    def _lazy_load_model(self) -> bool:
        """ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½ - å§‹ç»ˆä»æœ¬åœ°åŠ è½½"""
        if self._model is not None:
            return True

        if self._model_load_failed:
            return False

        try:
            global sentence_transformers, torch
            if sentence_transformers is None:
                import sentence_transformers
                import torch

            # CPUä¼˜åŒ–è®¾ç½®
            torch.set_num_threads(2)  # ä¿å®ˆçš„çº¿ç¨‹æ•°
            torch.set_grad_enabled(False)  # ç¦ç”¨æ¢¯åº¦è®¡ç®—

            # ç¡®ä¿æ¨¡å‹å¯ç”¨å¹¶è·å–æœ¬åœ°è·¯å¾„
            model_path = self.model_manager.ensure_model_available()

            logger.info(f"ğŸ“¥ Loading hybrid rerank model from: {model_path}")

            self._model = sentence_transformers.SentenceTransformer(model_path, device='cpu')

            # å¿«é€Ÿé¢„çƒ­
            _ = self._model.encode(["hybrid rerank warmup"], show_progress_bar=False)

            logger.info(f"âš¡ Hybrid rerank model loaded successfully with CPU optimizations")
            return True

        except Exception as e:
            logger.error(f"âŒ Hybrid rerank model loading failed: {e}")
            self._model_load_failed = True
            return False

    def _fast_preprocess_text(self, text: str) -> str:
        """é«˜æ€§èƒ½æ–‡æœ¬é¢„å¤„ç†"""
        if not text:
            return ""

        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
        text = HTML_TAG_RE.sub(' ', text)
        text = WHITESPACE_RE.sub(' ', text.strip())

        # å¿«é€Ÿé•¿åº¦é™åˆ¶
        return text[: self.max_length] if len(text) > self.max_length else text

    def _compute_embeddings_cpu_optimized(self, texts: list) -> np.ndarray | None:
        """CPUä¼˜åŒ–çš„åµŒå…¥è®¡ç®—"""
        if not texts:
            return None

        embeddings = []
        texts_to_compute = []
        indices_map = {}

        # å¿«é€Ÿç¼“å­˜æ£€æŸ¥
        for i, text in enumerate(texts):
            cached = self._semantic_cache.get(text)
            if cached is not None:
                embeddings.append(cached)
            else:
                embeddings.append(None)
                texts_to_compute.append(text)
                indices_map[len(texts_to_compute) - 1] = i

        if not texts_to_compute:
            return np.array(embeddings)

        try:
            # CPUå‹å¥½çš„å°æ‰¹é‡å¤„ç†
            computed_embeddings = []
            batch_size = min(self.cpu_batch_size, len(texts_to_compute))

            for i in range(0, len(texts_to_compute), batch_size):
                batch = texts_to_compute[i : i + batch_size]
                batch_emb = self._model.encode(
                    batch,
                    batch_size=len(batch),
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True,  # é¢„å…ˆæ ‡å‡†åŒ–
                )
                computed_embeddings.extend(batch_emb)

            # æ›´æ–°ç¼“å­˜å’Œç»“æœ
            for comp_idx, embedding in enumerate(computed_embeddings):
                orig_idx = indices_map[comp_idx]
                embeddings[orig_idx] = embedding
                self._semantic_cache.set(texts[orig_idx], embedding)

            return np.array(embeddings)

        except Exception as e:
            logger.error(f"âŒ Hybrid embedding computation failed: {e}")
            return None

    def _calculate_cpu_optimized_semantic_scores(self, query: str, corpus: list) -> np.ndarray | None:
        """CPUä¼˜åŒ–çš„è¯­ä¹‰åˆ†æ•°è®¡ç®—"""
        if not self._lazy_load_model():
            return None

        start_time = time.time()
        corpus_size = len(corpus)

        # ä¸¥æ ¼çš„å¤§å°é™åˆ¶
        if corpus_size > self.max_results_limit:
            logger.warning(f"âš ï¸ Truncating {corpus_size} to {self.max_results_limit} for CPU performance")
            corpus = corpus[: self.max_results_limit]
            corpus_size = len(corpus)

        try:
            # å¿«é€Ÿé¢„å¤„ç†
            processed_query = self._fast_preprocess_text(query)
            processed_corpus = [self._fast_preprocess_text(text) for text in corpus]

            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > self.cpu_timeout:
                logger.warning("â±ï¸ CPU timeout during preprocessing")
                return None

            # è·å–æŸ¥è¯¢åµŒå…¥
            query_embedding = self._semantic_cache.get(processed_query)
            if query_embedding is None:
                query_embedding = self._model.encode(
                    [processed_query], show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True
                )[0]
                self._semantic_cache.set(processed_query, query_embedding)

            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > self.cpu_timeout:
                logger.warning("â±ï¸ CPU timeout during query embedding")
                return None

            # è·å–æ–‡æ¡£åµŒå…¥
            doc_embeddings = self._compute_embeddings_cpu_optimized(processed_corpus)
            if doc_embeddings is None:
                return None

            # å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå·²ç»é¢„å…ˆæ ‡å‡†åŒ–ï¼‰
            similarities = np.dot(doc_embeddings, query_embedding)

            elapsed = time.time() - start_time
            throughput = corpus_size / elapsed if elapsed > 0 else 0

            if self.enable_debug_logs:
                logger.info(f"âš¡ Hybrid semantic processing: {elapsed:.2f}s, {throughput:.1f} docs/s")

            # æ€§èƒ½æ£€æŸ¥
            if elapsed > self.performance_threshold:
                logger.warning(f"ğŸŒ Slow hybrid processing: {elapsed:.2f}s")

            return similarities

        except Exception as e:
            logger.error(f"âŒ Hybrid semantic processing failed: {e}")
            return None

    def _fast_normalize_scores(self, scores: np.ndarray) -> np.ndarray:
        """å¿«é€Ÿåˆ†æ•°æ ‡å‡†åŒ–"""
        if len(scores) == 0:
            return scores

        min_val, max_val = np.min(scores), np.max(scores)
        if max_val > min_val:
            return (scores - min_val) / (max_val - min_val)
        return np.full_like(scores, 0.5)

    def _hybrid_score_combination(self, bm25_scores: np.ndarray, semantic_scores: np.ndarray) -> np.ndarray:
        """æ··åˆåˆ†æ•°ç»„åˆï¼šè¯­ä¹‰ä¼˜å…ˆçš„åŠ æƒèåˆ"""
        if semantic_scores is None:
            logger.warning("âš ï¸ Semantic scores unavailable, using BM25 only")
            return bm25_scores

        # å¿«é€Ÿæ ‡å‡†åŒ–
        bm25_norm = self._fast_normalize_scores(bm25_scores)
        semantic_norm = self._fast_normalize_scores(semantic_scores)

        # æ··åˆåŠ æƒç»„åˆï¼šè¯­ä¹‰ä¼˜å…ˆ
        combined = self.bm25_weight * bm25_norm + self.semantic_weight * semantic_norm

        if self.enable_debug_logs:
            logger.info(
                f"ğŸ”„ Hybrid scores: avg={np.mean(combined):.3f}, "
                f"BM25_avg={np.mean(bm25_norm):.3f}, "
                f"Semantic_avg={np.mean(semantic_norm):.3f}"
            )

        return combined

    def _create_fast_score_details(
        self,
        results: list,
        documents: list,
        bm25_scores: np.ndarray,
        semantic_scores: np.ndarray,
        combined_scores: np.ndarray
    ) -> list:
        """å¿«é€Ÿåˆ›å»ºåˆ†æ•°è¯¦æƒ…"""
        score_details = []
        doc_mapping = {doc_idx: rank for rank, doc_idx in enumerate(documents[0])}

        for doc_idx in range(min(len(results), self.max_results_limit)):
            result = results[doc_idx]
            title = getattr(result, 'title', f'Doc{doc_idx}') or f'Doc{doc_idx}'

            detail = ScoreDetails(doc_idx, title)

            if doc_idx in doc_mapping:
                rank = doc_mapping[doc_idx]
                detail.bm25_raw = float(bm25_scores[rank])
                detail.combined_score = float(combined_scores[rank])
                if semantic_scores is not None:
                    detail.semantic_raw = float(semantic_scores[rank])

            score_details.append(detail)

        return score_details

    def _display_fast_scores(self, score_details: list, query: str):
        """å¿«é€Ÿåˆ†æ•°æ˜¾ç¤º - æ··åˆæ’åºç»“æœ"""
        if not self.show_detailed_scores:
            return

        # ğŸ”„ æ··åˆæ’åºæ ‡é¢˜
        logger.info(f"ğŸ”„ HYBRID RERANK TOP {self.show_top_n_scores} RESULTS:")
        logger.info(f"ğŸ“Š Weights: Semantic {self.semantic_weight*100:.0f}% + BM25 {self.bm25_weight*100:.0f}%")
        logger.info("-" * 80)

        # å¿«é€Ÿæ’åºå’Œæ˜¾ç¤º
        sorted_details = sorted(score_details, key=lambda x: x.combined_score, reverse=True)

        for i, detail in enumerate(sorted_details[: self.show_top_n_scores]):
            logger.info(
                f"#{i+1:2d}: {detail.title}: "
                f"BM25={detail.bm25_raw:.3f}, "
                f"Semantic={detail.semantic_raw:.3f}, "
                f"Hybrid={detail.combined_score:.3f}"
            )

    def _cpu_optimized_position_multiplier(self, score: float) -> float:
        """ç®€åŒ–çš„ä½ç½®å€æ•°è®¡ç®—"""
        if score > 0.85:
            return 0.05
        elif score > 0.7:
            return 0.15
        elif score > 0.5:
            return 0.3
        else:
            return 0.8

    def _apply_hybrid_rerank(self, results: list, documents: list, normalized_scores: list):
        """åº”ç”¨æ··åˆé‡æ’åº"""
        for idx, doc_index in enumerate(documents[0][: len(results)]):
            if doc_index >= len(results):
                continue

            score = float(normalized_scores[idx])
            multiplier = self._cpu_optimized_position_multiplier(score)
            result = results[doc_index]

            if hasattr(result, 'positions') and result.positions:
                result.positions[0] = float(max(0.01, result.positions[0] * multiplier))
            else:
                result.positions = [float((doc_index + 1.0) / len(results) * multiplier)]

    def _process_hybrid_results(self, results: list, query: str) -> None:
        """æ··åˆé‡æ’åºçš„ç»“æœå¤„ç†ä¸»æµç¨‹"""
        if len(results) < 2:
            return

        # ä¸¥æ ¼é™åˆ¶å¤„ç†æ•°é‡
        if len(results) > self.max_results_limit:
            results = results[: self.max_results_limit]
            logger.warning(f"âš ï¸ Limited to {self.max_results_limit} results for CPU performance")

        start_time = time.time()

        # å¿«é€Ÿæ„å»ºè¯­æ–™åº“
        corpus = []
        for result in results:
            text_parts = []
            if hasattr(result, 'title') and result.title:
                text_parts.append(result.title)
            if hasattr(result, 'content') and result.content:
                text_parts.append(result.content[:100])  # é™åˆ¶contenté•¿åº¦
            corpus.append(" ".join(text_parts))

        # è·å–åœç”¨è¯ï¼ˆç¼“å­˜ï¼‰
        if not hasattr(self, '_cached_stopwords'):
            self._cached_stopwords = {
                word
                for name, value in stopwords_module.__dict__.items()
                if name.startswith("STOPWORDS_") and isinstance(value, tuple)
                for word in value
            }
        stopwords = self._cached_stopwords

        # BM25å¤„ç†
        corpus_tokens = bm25s.tokenize(corpus, stopwords=stopwords)
        query_tokens = bm25s.tokenize(query, stopwords=stopwords)

        retriever = bm25s.BM25()
        retriever.index(corpus_tokens)
        documents, bm25_scores = retriever.retrieve(query_tokens, k=len(corpus), return_as="tuple", show_progress=False)

        bm25_time = time.time() - start_time

        # è¯­ä¹‰å¤„ç†
        semantic_start = time.time()
        semantic_scores = self._calculate_cpu_optimized_semantic_scores(query, corpus)
        semantic_time = time.time() - semantic_start

        if bm25_scores is not None and len(bm25_scores) > 0:
            # æ··åˆåˆ†æ•°ç»„åˆ
            combined_scores = self._hybrid_score_combination(bm25_scores[0], semantic_scores)
            normalized_scores = self._fast_normalize_scores(combined_scores).tolist()

            if normalized_scores:
                # åº”ç”¨æ··åˆé‡æ’åº
                self._apply_hybrid_rerank(results, documents, normalized_scores)

                # å¦‚æœå¯ç”¨ï¼Œæ˜¾ç¤ºåˆ†æ•°
                if self.show_detailed_scores:
                    score_details = self._create_fast_score_details(
                        results, documents, bm25_scores[0], semantic_scores, combined_scores
                    )
                    self._display_fast_scores(score_details, query)

                total_time = time.time() - start_time

                logger.info(
                    f"ğŸ”„ Hybrid rerank processing: {total_time:.2f}s total "
                    f"(BM25: {bm25_time:.2f}s, Semantic: {semantic_time:.2f}s)"
                )

                self._performance_stats["cpu_optimized_calls"] += 1
                if self.enable_fast_mode:
                    self._performance_stats["fast_mode_calls"] += 1

    def post_search(self, request: "SXNG_REQUEST", search: "SearchWithPlugins") -> EngineResults:
        results = search.result_container.get_ordered_results()

        if len(results) < 2:
            return search.result_container

        query = search.search_query.query

        logger.info(f"ğŸ”„ Hybrid rerank processing: {len(results)} results")

        self._process_hybrid_results(results, query)

        # ç®€åŒ–æ€§èƒ½ç»Ÿè®¡
        self._performance_stats["total_calls"] += 1
        if self._performance_stats["total_calls"] % 10 == 0:
            hit_rate = self._semantic_cache.get_hit_rate()
            logger.info(
                f"ğŸ“Š Hybrid rerank stats: {self._performance_stats['total_calls']} calls, "
                f"cache hit rate: {hit_rate:.1f}%"
            )

        return search.result_container

    def clear_cache(self) -> None:
        """æ¸…ç†ç¼“å­˜"""
        self._semantic_cache.cache.clear()
        self._semantic_cache.access_times.clear()
        logger.info("ğŸ§¹ Hybrid rerank cache cleared")

    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.model_manager.get_model_info()
